{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eddyxu/rikai/blob/lei%2Funcertainty_sampling/notebooks/UncertaintySampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea741a14-19ba-4157-911f-2511d24554c6",
      "metadata": {
        "id": "ea741a14-19ba-4157-911f-2511d24554c6"
      },
      "source": [
        "# Uncertainty Sampling\n",
        "\n",
        "Uncertainty Sampling is one [Active Learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning))\n",
        "strategy to use the uncertainty in model detection to find examples to be labelled.\n",
        "\n",
        "![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)(https://colab.research.google.com/github/eto-ai/rikai/]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V\n",
        "!nvidia-smi\n",
        "!df -h"
      ],
      "metadata": {
        "id": "xqjPxJYsltqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadb0982-b27b-4f3a-d7d0-7bcfb104ae4a"
      },
      "id": "xqjPxJYsltqk",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   43G   65G  40% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  817M  59% /sbin/docker-init\n",
            "tmpfs           6.4G   44K  6.4G   1% /var/colab\n",
            "/dev/sda1        81G   47G   34G  59% /etc/hosts\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rikai[mlflow,torch]"
      ],
      "metadata": {
        "id": "iEv4UyULmHCT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f3df2ba-5c31-4ab5-bd65-efb4950c45c6"
      },
      "id": "iEv4UyULmHCT",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rikai[mlflow,torch]\n",
            "  Downloading rikai-0.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 40 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 71 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 81 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 92 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 5.8 MB/s \n",
            "\u001b[33mWARNING: rikai 0.1.0 does not provide the extra 'torch'\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rikai[mlflow,torch]) (1.19.5)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 30 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 40 kB 38.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 51 kB 39.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 61 kB 42.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 71 kB 41.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 81 kB 43.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 92 kB 46.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 102 kB 47.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112 kB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from rikai[mlflow,torch]) (4.3.3)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rikai[mlflow,torch]) (7.1.2)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from rikai[mlflow,torch]) (2.13.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from rikai[mlflow,torch]) (1.1.5)\n",
            "Collecting pyspark==3.1.2\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 58 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from rikai[mlflow,torch]) (3.13)\n",
            "Collecting ipython!=8.0.0,>=7.31.1\n",
            "  Downloading ipython-7.31.1-py3-none-any.whl (792 kB)\n",
            "\u001b[K     |████████████████████████████████| 792 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from rikai[mlflow,torch]) (2.23.0)\n",
            "Collecting pyarrow>=6.0\n",
            "  Downloading pyarrow-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.6 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting mlflow==1.22\n",
            "  Downloading mlflow-1.22.0-py3-none-any.whl (15.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.5 MB 192 kB/s \n",
            "\u001b[?25hCollecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.16.2.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (3.17.3)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (4.10.1)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (1.4.31)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 31.4 MB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.7-py3-none-any.whl (17 kB)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (1.1.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (0.3)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (2018.9)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (0.4.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (7.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow==1.22->rikai[mlflow,torch]) (21.3)\n",
            "Collecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.2 MB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic<=1.4.1->mlflow==1.22->rikai[mlflow,torch]) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow==1.22->rikai[mlflow,torch]) (0.8.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow==1.22->rikai[mlflow,torch]) (1.15.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow==1.22->rikai[mlflow,torch]) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow==1.22->rikai[mlflow,torch]) (3.7.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (57.4.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (0.1.3)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.26-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (0.18.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (4.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython!=8.0.0,>=7.31.1->rikai[mlflow,torch]) (0.2.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->rikai[mlflow,torch]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->rikai[mlflow,torch]) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->rikai[mlflow,torch]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->rikai[mlflow,torch]) (3.0.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow==1.22->rikai[mlflow,torch]) (1.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow==1.22->rikai[mlflow,torch]) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow==1.22->rikai[mlflow,torch]) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow==1.22->rikai[mlflow,torch]) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow==1.22->rikai[mlflow,torch]) (2.0.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->rikai[mlflow,torch]) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->rikai[mlflow,torch]) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->rikai[mlflow,torch]) (5.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow==1.22->rikai[mlflow,torch]) (3.0.7)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow==1.22->rikai[mlflow,torch]) (0.13.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, pyspark, alembic, databricks-cli\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=fbee40aa81e9c40bc3a33bfe25d211fee38fb276bc769ff0ce93fb9dd0a3a6ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880769 sha256=7d1be214302da161c8ac876df8abbb34b623f1108c28029ee7b4a0df5e274529\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158171 sha256=28106888db95ee985fcce3f664f0fbdd8d17bbcd13419b9b61fb81a7d2ab15af\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.2-py3-none-any.whl size=106811 sha256=74e560a48cf8b6ba174d5410cc429004a58e560a3d7b7639a84cdeed153ef646\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/5c/ed/e1ce20a53095f63b27b4964abbad03e59cf3472822addf7d29\n",
            "Successfully built antlr4-python3-runtime pyspark alembic databricks-cli\n",
            "Installing collected packages: smmap, websocket-client, python-editor, py4j, prompt-toolkit, Mako, gitdb, querystring-parser, pyyaml, pyspark, pyarrow, prometheus-flask-exporter, opencv-python-headless, ipython, gunicorn, gitpython, docker, databricks-cli, antlr4-python3-runtime, alembic, rikai, mlflow\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.26 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.31.1 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.1.6 alembic-1.4.1 antlr4-python3-runtime-4.8 databricks-cli-0.16.2 docker-5.0.3 gitdb-4.0.9 gitpython-3.1.26 gunicorn-20.1.0 ipython-7.31.1 mlflow-1.22.0 opencv-python-headless-4.5.5.62 prometheus-flask-exporter-0.18.7 prompt-toolkit-3.0.26 py4j-0.10.9 pyarrow-6.0.1 pyspark-3.1.2 python-editor-1.0.4 pyyaml-6.0 querystring-parser-1.2.4 rikai-0.1.0 smmap-5.0.0 websocket-client-1.2.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9a96c36f-1274-4f6e-b46a-34a7682db93d",
      "metadata": {
        "id": "9a96c36f-1274-4f6e-b46a-34a7682db93d"
      },
      "outputs": [],
      "source": [
        "# From https://pytorch.org/vision/0.11/models.html#object-detection-instance-segmentation-and-person-keypoint-detection\n",
        "\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "be111e0a-1e0c-4830-92aa-822b4d196d46",
      "metadata": {
        "id": "be111e0a-1e0c-4830-92aa-822b4d196d46"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from pyspark.sql import SparkSession\n",
        "from rikai.spark.utils import get_default_jar_version\n",
        "\n",
        "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
        "\n",
        "rikai_version = get_default_jar_version(use_snapshot=False)\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .config(\"spark.jars.packages\", f\"ai.eto:rikai_2.12:{rikai_version}\")\n",
        "    .config(\n",
        "        \"spark.sql.extensions\",\n",
        "        \"ai.eto.rikai.sql.spark.RikaiSparkSessionExtensions\",\n",
        "    )\n",
        "    .config(\n",
        "        \"spark.rikai.sql.ml.registry.mlflow.tracking_uri\",\n",
        "        MLFLOW_TRACKING_URI,\n",
        "    )\n",
        "    .config(\"spark.executor.memory\", \"8g\")\n",
        "    .config(\"spark.driver.memory\", \"4g\")\n",
        "    .master(\"local[2]\")\n",
        "    .getOrCreate()\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e92d69d7-e3cf-4a1e-b66b-7f86753e2ad4",
      "metadata": {
        "id": "e92d69d7-e3cf-4a1e-b66b-7f86753e2ad4"
      },
      "source": [
        "# Preparing data\n",
        "\n",
        "Use rikai.contrib.coco.convert to create a Coco Rikai dataset under \"./coco\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "coco_dir = Path(\"./coco\")\n",
        "if not coco_dir.exists() and True:\n",
        "  !mkdir -p coco\n",
        "  !wget http://images.cocodataset.org/zips/val2017.zip -O coco/val2017.zip\n",
        "  !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco/annotations.zip\n",
        "  !cd coco && find . -name '*.zip' -exec unzip {} \\; && rm *.zip\n",
        "\n",
        "from rikai.contrib.datasets.coco import convert\n",
        "\n",
        "df = convert(spark, \"coco\")\n",
        "df.write.saveAsTable(\"coco\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2rZLFflsKaj",
        "outputId": "c343a4cb-5512-4e43-dc71-f58efb22c1a6"
      },
      "id": "y2rZLFflsKaj",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=19.08s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.60s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5e809631-ade5-4838-850c-c73d837691ed",
      "metadata": {
        "id": "5e809631-ade5-4838-850c-c73d837691ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8992bb6-80cb-40c8-b912-f8f09cff1fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-----------+\n",
            "|database|tableName|isTemporary|\n",
            "+--------+---------+-----------+\n",
            "| default|     coco|      false|\n",
            "+--------+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SHOW TABLES\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "37b4d857-30d6-4a83-a0e6-bcec4a039731",
      "metadata": {
        "tags": [],
        "id": "37b4d857-30d6-4a83-a0e6-bcec4a039731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53eabee9-681b-4354-8681-410f9285b248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- image_id: long (nullable = true)\n",
            " |-- annotations: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- label_id: integer (nullable = true)\n",
            " |    |    |-- label: string (nullable = true)\n",
            " |    |    |-- area: float (nullable = true)\n",
            " |    |    |-- bbox: box2d (nullable = true)\n",
            " |-- image: image (nullable = true)\n",
            " |-- split: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from coco\").printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dfd14ea7-e4a8-42da-9ad0-bdb12fcc4dbf",
      "metadata": {
        "id": "dfd14ea7-e4a8-42da-9ad0-bdb12fcc4dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a08e89cd-749c-41c0-bf8e-226d19c6c8d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ab24f9b51700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrikai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mssd300_vgg16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrikai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSSDClassScoresExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrikai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOUTPUT_SCHEMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rikai.contrib.torch.inspect'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import rikai\n",
        "from torchvision.models.detection.ssd import ssd300_vgg16\n",
        "from rikai.contrib.torch.inspect.ssd import SSDClassScoresExtractor\n",
        "from rikai.contrib.torch.detections import OUTPUT_SCHEMA\n",
        "\n",
        "ssd = ssd300_vgg16(pretrained=True)\n",
        "class_scores_extractor = SSDClassScoresExtractor(ssd, topk_candidates=90)\n",
        "\n",
        "print(OUTPUT_SCHEMA)\n",
        "\n",
        "with mlflow.start_run():\n",
        "    rikai.mlflow.pytorch.log_model(\n",
        "        ssd, \n",
        "        \"model\", \n",
        "        OUTPUT_SCHEMA,\n",
        "        pre_processing=\"rikai.contrib.torch.transforms.ssd.pre_processing\",\n",
        "        post_processing=\"rikai.contrib.torch.transforms.ssd.post_processing\",\n",
        "        registered_model_name=\"ssd\"\n",
        "    )\n",
        "with mlflow.start_run():\n",
        "    rikai.mlflow.pytorch.log_model(\n",
        "        class_scores_extractor,\n",
        "        \"model_scores\",\n",
        "        SSDClassScoresExtractor.SCHEMA,\n",
        "        pre_processing=\"rikai.contrib.torch.inspect.ssd.class_scores_extractor_pre_processing\",\n",
        "        post_processing=\"rikai.contrib.torch.inspect.ssd.class_scores_extractor_post_processing\",\n",
        "        registered_model_name=\"class_scores\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499dbce9-eb58-4ee3-b3f9-a426cea2aa28",
      "metadata": {
        "id": "499dbce9-eb58-4ee3-b3f9-a426cea2aa28"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"CREATE OR REPLACE MODEL ssd OPTIONS (batch_size=128) USING 'mlflow:/ssd'\")\n",
        "spark.sql(\"CREATE OR REPLACE MODEL class_scores OPTIONS (batch_size=128) USING 'mlflow:/class_scores'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be378fa8-653d-4fb9-868f-d80c17f8e074",
      "metadata": {
        "id": "be378fa8-653d-4fb9-868f-d80c17f8e074"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SHOW MODELS\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4653ca7b-a3b6-4da1-a4a7-ed69b30e804e",
      "metadata": {
        "id": "4653ca7b-a3b6-4da1-a4a7-ed69b30e804e"
      },
      "source": [
        "# Least Confidence\n",
        "\n",
        "**Least Confidence** looks for predicted labels with the lowest degree of confidence\n",
        "\n",
        "$$ 1 - P(y_1 | x) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7283817-dbc7-4b82-bb7d-cddafd4d2e4f",
      "metadata": {
        "id": "d7283817-dbc7-4b82-bb7d-cddafd4d2e4f"
      },
      "outputs": [],
      "source": [
        "df = spark.sql(\"\"\"\n",
        "SELECT image_id, image, explode(ML_PREDICT(ssd, image)) AS ssd FROM (\n",
        "    SELECT image_id, image FROM coco LIMIT 1000\n",
        ") ORDER BY ssd.score ASC\n",
        "\"\"\").cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724d3166-4a93-4699-8c7a-40d78bc06b03",
      "metadata": {
        "id": "724d3166-4a93-4699-8c7a-40d78bc06b03"
      },
      "outputs": [],
      "source": [
        "from rikai.viz import Text\n",
        "\n",
        "for row in df.take(3):\n",
        "    text = COCO_INSTANCE_CATEGORY_NAMES[row.ssd.label_id]\n",
        "    display(row.image \n",
        "        | row.ssd.box@{\"color\": \"yellow\", \"width\": 3} \n",
        "        | Text(f\"{text} | {row.ssd.score:.3f}\", (row.ssd.box.xmin, row.ssd.box.ymax + 3))@{\"color\": \"yellow\"}\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e944365-a15c-4bd8-8c5f-0175fa9f3cef",
      "metadata": {
        "id": "5e944365-a15c-4bd8-8c5f-0175fa9f3cef"
      },
      "source": [
        "# Least Margin of Confidence\n",
        "\n",
        "**Margin of Confidence** looks for training examples with the lowest difference between most likely and second most likely labels. Intuitively, it gives insights into where the model is confused the most.\n",
        "\n",
        "$$ P(y_1 | x) - P(y_2 | x) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46ee94f-1093-4267-9e4c-3cd4a7fe41dd",
      "metadata": {
        "id": "c46ee94f-1093-4267-9e4c-3cd4a7fe41dd"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT image_id, image, detection, detection.scores[0] - detection.scores[1] as margin FROM (\n",
        "    SELECT image_id, image, explode(ML_PREDICT(class_scores, image)) AS detection FROM (\n",
        "        SELECT image_id, image FROM coco LIMIT 100\n",
        "    )\n",
        ") ORDER BY margin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d715eaf4-4dae-4122-8202-ef13fe69301d",
      "metadata": {
        "id": "d715eaf4-4dae-4122-8202-ef13fe69301d"
      },
      "outputs": [],
      "source": [
        "df.printSchema()\n",
        "df.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a5ef27-9657-40e1-9e86-114d5470ee90",
      "metadata": {
        "id": "d4a5ef27-9657-40e1-9e86-114d5470ee90"
      },
      "outputs": [],
      "source": [
        "first = df.first()\n",
        "label1 = COCO_INSTANCE_CATEGORY_NAMES[first.detection.label_ids[0]]\n",
        "label2 = COCO_INSTANCE_CATEGORY_NAMES[first.detection.label_ids[1]]\n",
        "text = f\"{label1} - {label2} = {first.margin}\"\n",
        "box = first.detection.box\n",
        "(\n",
        "    first.image \n",
        "    | box@{\"color\": \"yellow\", \"width\": 3} \n",
        "    | Text(text, (box.xmin, box.ymax))@{\"color\": \"yellow\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da7452b6-6b7d-4e14-acf3-28b1354d2eac",
      "metadata": {
        "id": "da7452b6-6b7d-4e14-acf3-28b1354d2eac"
      },
      "source": [
        "# Entropy\n",
        "\n",
        "**Entropy** observing the average level of uncertainty over all the labels.\n",
        "\n",
        "$$ \\frac{-\\sum_{i=1}^{n}P(y_i | x)log_{2}P(y_i | x)}{log_2{n}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e9a3a1-776f-42e4-ae82-93cffb807508",
      "metadata": {
        "id": "d4e9a3a1-776f-42e4-ae82-93cffb807508"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "from scipy.stats import entropy as scipyEntropy\n",
        "\n",
        "@udf(returnType=FloatType())\n",
        "def entropy(arr) -> float:\n",
        "    return float(scipyEntropy(arr))\n",
        "\n",
        "spark.udf.register(\"entropy\", entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5fb05ec-7818-42f5-aea1-7a0a9f922718",
      "metadata": {
        "id": "e5fb05ec-7818-42f5-aea1-7a0a9f922718"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT image_id, image, detection, entropy(detection.scores) as entropy FROM (\n",
        "    SELECT image_id, image, explode(ML_PREDICT(class_scores, image)) AS detection FROM (\n",
        "        SELECT image_id, image FROM coco LIMIT 1000\n",
        "    )\n",
        ") ORDER BY entropy DESC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb324233-7729-4253-bbaf-e77026bf3f9d",
      "metadata": {
        "id": "fb324233-7729-4253-bbaf-e77026bf3f9d"
      },
      "outputs": [],
      "source": [
        "df.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159fb6c8-6902-4655-9c7f-1d6cdb30a4fa",
      "metadata": {
        "id": "159fb6c8-6902-4655-9c7f-1d6cdb30a4fa"
      },
      "outputs": [],
      "source": [
        "first = df.first()\n",
        "text = COCO_INSTANCE_CATEGORY_NAMES[first.detection.label_ids[0]]\n",
        "box = first.detection.box\n",
        "print(box)\n",
        "(\n",
        "    first.image \n",
        "    | box@{\"color\": \"yellow\", \"width\": 3} \n",
        "    | Text(text, (box.xmin, box.ymax))@{\"color\": \"yellow\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38aa348e-1b61-4907-9a18-6eba553c497e",
      "metadata": {
        "id": "38aa348e-1b61-4907-9a18-6eba553c497e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Copy of UncertaintySampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}